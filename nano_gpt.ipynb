{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5hjCcLDr2WC"
      },
      "outputs": [],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnnEAjf8TT54",
        "outputId": "14a55f95-f003-4bc7-881b-d90cee35a435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 500\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/hafez gpt/HafezFull.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "# itos = { i:ch for i,ch in enumerate(chars) }\n",
        "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        ")\n",
        "\n",
        "# def load_dataset(path, tokenizer):\n",
        "\n",
        "#     dataset = TextDataset(\n",
        "#         tokenizer=tokenizer, file_path=path, block_size=256\n",
        "#     )\n",
        "#     n = int(0.9*len(dataset)) # first 90% will be train, rest val\n",
        "#     train_dataset = dataset[:n]\n",
        "#     test_dataset = dataset[n:]\n",
        "\n",
        "#     return train_dataset, test_dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "# train_dataset, test_dataset = load_dataset(\n",
        "#     \"/content/drive/MyDrive/hafez gpt/HafezFull.txt\", tokenizer\n",
        "# )\n",
        "encode = tokenizer.encode\n",
        "decode = tokenizer.decode\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z42fRf_iHXUa",
        "outputId": "b6764989-9927-48c8-87d4-f8b05b36b56b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.426344 M parameters\n",
            "step 0: train loss 10.2706, val loss 10.2498\n",
            "step 100: train loss 6.1396, val loss 6.3704\n",
            "step 200: train loss 5.8092, val loss 6.1226\n",
            "step 300: train loss 5.6405, val loss 5.9676\n",
            "step 400: train loss 5.5449, val loss 5.8904\n",
            "step 499: train loss 5.4541, val loss 5.8585\n",
            "<unk>م مرا حسن خاک راه<unk>ر<unk>م تو و پ<unk>ب می وی من رهگذر ز وصل از حمله دهم مد ار در عمرم غم در عجوز قدح به جان وفا برفت<unk>ب سراپرده مرا در شمش از شب خسته و آغوشم که <unk>ن شعر به طوفان آرای  بی در در <unk>ن<unk>ن زهی می‌ درون برو عقل دعای صلاح<unk>نت م سلطان بب<unk> برود ز چشمم که ب<unk>اد جوان شد که خاص ری سه آشکار بر بد پ<unk>غ ب<unk>ار کوکب<unk>نه نس ب<unk>ر حافظ را چنانم خاک ا<unk>ن برگرفت چ<unk>بت که بو و ساغر ص<unk>ش دور طوطی و حجاز می من ماست موم<unk>لی و ره ب<unk>نه حضرت ما راان جم<unk>ن مدد معما گر قر جلال می‌چه خواب نمی‌کرد خبر وبند و <unk>رم داد خدمت تو ابروی برود دوست ش<unk>م جامه ز ف<unk>ر مرا مکن کس کرده شاهد حافظ چه آباد و نی پروانه چاک کرد که رهگذر خبر از از بگذر بر شمع حافظ گفت از شمال <unk>ز<unk> طر<unk>ام با بوی کن که چه بنده آر و زمان و موزون انتخاب تو صدرش تو ضم<unk>ار نه در موم باد شد شد دلش شعای معاش خارا خوشدرد شاهی جا ثنا آی باز کنم ما به ق<unk>قم برداران سرو برگرفت کنم وفا دوست به نپرس را زان کند دلم گو<unk>ا ع<unk>م دل چنان جا چند و آواز پ<unk>ن جهان باشد برفت کرد اکنون چشم که<unk>ت ای مهر تو ای خطا اقبالی معما ندارد ر<unk> زده جلوه در لاله پ<unk> الکی ندهند تو که در شد داد سرو دل دوستیان از به<unk>ه جفا به المپیا تو هم من دانسته رب به سراب دوران از بعد عارضش ز خوشا عشق که راهیان غافل نالان خون بسا چون مپرس که کنند باشد ل<unk>ل چه ساغر که رخ آگاه گمراه گوهر وز گفت دل را نقطه جهان در<unk> ده هندوی و نافه سر دل و بو می‌ای ار را غمت<unk>وان‌ای رو ماست خرام کا<unk>اد مساز پند که در مفت عمر رفتار خود کوش نه که آمارها چو بساز وفای بزن بگو<unk>م مستم آزاد از رکاب گدا نگفت و دست سلسله هیچگاه دور بنده من بودقه نامه به در ماند و آصف حوری را درحت ز حد<unk>اران مقام نام توبه گر خود که چه نبود ر<unk>ن صبا ز او پاک بازتر رفت از دست چه مج سوخت کار کز<unk>ق<unk>ت باد<unk>اد بسیت که ابن خدا من بود کرد که سوی خدام خور جهان روی زهد حال آ<unk>ن رو شعر<unk>ه ا<unk>ن بود بر دوست تو اهرمنافت به گفتم نه کرم رس<unk>ده می‌ا عشق صبا ما کس روی به پامال ازر پرخاشگری بخش می‌رود<unk>ش است بر ن<unk>م د<unk>ن به عنان مردمان وصال بی‌با بد تو باعظ عزم دلشاد ناز آن روان ز دست شد بمان می‌ نداند در زان بپوشان<unk>نه غ<unk>م صبوری همچون عوام در<unk>دا عشقش حکم می‌ حاسد نتوانم نمی‌دهند تو<unk>ث که حشمتدن رستگاری شوم آن انه س<unk>ک صد ز من خواند نمی فروش که ابروی خواجه چشم و ذره اگر سر آن دانا که در نخوت به امان<unk>ل است راه گره در مبتلا در آبف گدا<unk>دستعظمم از گنج که باشد کنان از کارگاه جانان به مدرسه بر أن س<unk>چ هدا کند پرست توام ملامت رفت خود ما آن که هر من هر دهد گوهر <unk>ال ز<unk>حت ز و در دلی به کام ببری از لب و ن<unk>ش ز ندارد باشد دربند دگر کس بر ح<unk>اد طمع حافظ کنمش ار طرب ز روی به سپردها<unk>م نرستوس هر آرزو نی است گر کجاست جان دلم بود آن ا<unk>د روح رفتم من نفس نرگست <unk>ست شهی تو حسن ندارد ناچار آن شاهی سرو بنه گرفت که جهل دگرگه سهم ا<unk>ا نفس او شد نهاد مرا پیگرد کرد نوش راا صد کنم سای ببرد هر عرصه گفت که هوا در زحمت تو طرهه به عمل به حج ندارد شهپر از گه بود<unk>ن خوبان<unk>المجالس م عمر<unk>ر خدا<unk>مان بلبلی گاه<unk>ر چاه ساخت روزگاران درد رسسر<unk>نهی مفروشخورده مهنی <unk>رون کنم گلشن به گل<unk>نان آن حدم ل<unk>د به دست نگاه رهی هزار نزاع مهلت فرو شو<unk>م فدا آنانطا<unk>نه جا ندانم آباد گفت ای به سارا چشمی <unk>ن<unk>د سخنان کرد نگار چه عکس داده گر حافظ پناه ببرد و گوشه و را حافظنی باد شب که تو جوی ضعیفی چون فهم کارخانه عارض چو می‌داری به سخن تا از رخ جان ننوشت نثار حد نهند<unk>ن به رغبت خواب کهکشم که در<unk>ی من گران ع<unk>نه آب چو <unk>ار عقل حر<unk>ش خمار م<unk>اض سلام سر بس دن<unk> دارو قول دلم نبود که که اشت شدهگه پردهپ که بر باد جلال در<unk> جبهه تو هست کردم که گرد دوش مکارم ن<unk>ت فرو خوش برگ<unk>رک‌دهد<unk>ده سری اوضاع گر شو از همه در غمش می‌اش است در ع<unk>د که چه بهشت بار به مست شد کوش عم بی‌کرد او حسن سر جهان آ<unk>ن دور گرم خون پاکی استخوان را جرعه<unk>د و روستاآباد غم<unk>ار حدش هزار خوبه لعل در جان باده توست تا خاموشست بنواز گوش که باده نمی‌ است درآ<unk>ار نرگسن وز و گوشهفروشی ای خواند پ<unk>اله خالی خ<unk>ش من تو با اهل دلدار بب<unk> امام صبا قد دل را راه همیش نص<unk>ه بر ساق سخن چه جانان گفت شب خندان کز غم مراد پوش ایش خرم پ<unk>ر هر گدا<unk>له نا ا<unk>ست ب<unk>ن زبان دارم نرگسکشم شاخ گوی ادراک و آم را قرعه خون لاله به چشم افتاده بر ا<unk>ن گو<unk>نه کرامی ب<unk>غ چو سعدپف مکن عشاقی ا<unk>ن دل گ<unk>به گشادی کن بر سر ا<unk>ست به نظر به صنوبر<unk>ش چو دوستان رخت نهان حافظ در نخواهم کمال به روان حد ن<unk>تی نافه <unk>ن<unk>زد دلان که  انداختی چو رفت می با می‌کرد آن گره در مباد به نبود و بم<unk>ا خواهم پرور دمی و  مژگان چو ز مناق خود زهد الع طعنه برهاندی نقش قال به نمااز روان لعل ز<unk>م نم طبنوشد<unk>ب<unk>ست د<unk>ب<unk>راز که بهر مر عشرت در مرغان به خود پ<unk>ن ن<unk>ک<unk> سیرین بود‌ متین سلطنت قصد درخوری و چمن وروت ناز گردون هست بر چشم لعل رفت سحر کن<unk>ش زدم بر دللام مشتی من کوی نرود و من و هلاک کردیسر باده کان غافل س<unk>د همچو برخاست دم‌ای دم و سل<unk>ر خون بسوخت کاری ایکرد کویات بست <unk>افت بر حسرت ای هواداری حافظش دوش هر بر کشم زم <unk>ستو مست جز که چون بگ اشارت <unk>نم زنده سلطانیلام درم کجاست گفت کلکش<unk>نم پادشه ه<unk>د خانقاه همه که از د<unk>شم از برقص ضع<unk>د تماشای که صبح آخر می‌کنی و سوگواری پنهان شاهد چشم کهم نس<unk> آثار<unk>ان خرقه بگو است<unk>م مجوی کمانی کش بردی کرد و نگه عالم ز را باران کنم ز هجران دارد بسوز بود بمرد آش ماست گر ای دور که بر ع<unk>ا که شود بود به زن بر ابرام بر ا<unk>ا<unk>زد از زلفش ب<unk>اد گ<unk>ن خلق چو غ<unk>ن همچو ای من همتی حافظت مبادا وخرم سجاده<unk>ن بماندی روم<unk>ش بر ب<unk>نه حافظ تو منظر لب خود ش ای بادخواهم مگو روزی<unk>مان از دل گ<unk>ش عرصه‌ن قوت ریلم ازگردونخرد<unk>ش شرب<unk>ستی منکش مباحث همچون به بازار و امروز آن ساقی دوست عز<unk>شه سحر سری گنج طاق کهان اثباتش بر دست هجرانیپسند چو از درج چنان من ز  لعل از سلطانی زان پخت افسر درتاب را بروم چن<unk>ن که شما راه کاندر احمر<unk>نه نرگس در کنم نهان شراب ع<unk>لک حافظ که دانی ما گر نه خرابات مشتاقان کوته به خرمن خود حافظ می‌ نای پف نمی‌ رهزن <unk>شم از مستان ز سم رفت سورهش چ<unk>ار اوست ب<unk>ار که دعایرده<unk>دارت کنم نه که دولت شکل ای بس در شمعم شر و زن همه آن تو را کند صوف<unk>م قص وگران گر تو در سر شمعم و خوش جز حاشا صبا و زستان از ا<unk>ر ارمه قامت ز مخور عشق آزادگی و دام و متابوار دل به مند جرعه گر بنواز از سر ببست کاردانی سابقه ب<unk>مان کجاست و باغ جهان چه فراغست که ساقی برابر <unk>باش ل<unk>نه جاودان پ<unk> همدستی و جلوه خود سوسن پری اندازد ش<unk>دنش همه در دل <unk>لان بد شکر جوان بهگ<unk>ر ر<unk>اد بسوز رف<unk>د به دولت طی ما رسم با است شکن و جام درخواستم می‌ای به قبا زهد چاره کار بخ را بود که دهان کوی چه ح<unk>ح نبودلی ب<unk>نه بن دارم ده حافظ و باغ کشته شن<unk>ن صراحی می‌کشم حافظ و مر مگر هوا کنم که بود ور هر غول رقص مجوبچهی برود دلم زهی به بخواهم پرست آگاهستای و جام صبا نه آنانگ لابه تو گنج بهشت<unk>ز انور‌م بر س<unk>اض که پر<unk>م<unk>رم مرا رب بهآ<unk>ن و ناز<unk>ش وقتی را چون فلک س<unk>اد د<unk>ر خم نشد است ساغر م<unk>قت در<unk>نه باغبان نوشند پری رخ و خزان پ<unk>اه گذر داند زان زند عارض تا از رندی<unk>چ عارف‌ا<unk>ن که خرام از می‌ام دا کسب کوی رود اوست مژگان مجلس مشک<unk>ل نی<unk> مشاوران حافظ به آستان که ده حسن خم که تاب سرزنش‌ای سر آن که کلام خود دله ز طعن<unk>ض پای ندارد <unk> یوم خوش باد باشد و پند روان من خدا دل کشته گر کجاست درغ ای کام که وضو و هست نظم دولتی گردون عاشقان به گر ن<unk>ز به روانه<unk>د در لب<unk>ب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/hafez gpt/HafezFull.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "600e2c66-30be-4b79-d6e3-a0f5c34953e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  272131\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "49c06d51-7caa-4d98-f21d-7916b5d1518f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الا يا ايها الساقی ادر کاسا و ناولها\n",
            "که عشق آسان نمود اول ولی افتاد مشکل‌ها\n",
            "به بوی نافه‌ای کاخر صبا زان طره بگشايد\n",
            "ز تاب جعد مشکينش چه خون افتاد در دل‌ها\n",
            "مرا در منزل جانان چه امن عيش چون هر دم\n",
            "جرس فرياد می‌دارد که بربنديد محمل‌ها\n",
            "به می سجاده رنگين کن گرت پير مغان گويد\n",
            "که سالک بی‌خبر نبود ز راه و رسم منزل‌ها\n",
            "شب تاريک و بيم موج و گردابی چنين هايل\n",
            "کجا دانند حال ما سبکباران ساحل‌ها\n",
            "همه کارم ز خود کامی به بدنامی کشيد آخر\n",
            "نهان کی ماند آن رازی کز او سازند محفل‌ها\n",
            "حضوری گر همی‌خواهی از او غايب مشو حافظ\n",
            "متی ما تلق من تهوی دع الدنيا و اهملها\n",
            "صلاح کار کجا و من خراب کجا\n",
            "ببين تفاوت ره کز کجاست تا به کجا\n",
            "دلم ز صومعه بگرفت و خرقه سالوس\n",
            "کجاست دير مغان و شراب ناب کجا\n",
            "چه نسبت است به رندی صلاح و تقوا را\n",
            "سماع وعظ کجا نغمه رباب کجا\n",
            "ز روی دوست دل دشمنان چه دريابد\n",
            "چراغ مرده کجا شمع آفتاب کجا\n",
            "چو کحل بينش ما خاک آستان شماست\n",
            "کجا رويم بفرما از اين جناب کجا\n",
            "مبين به سيب زنخدان که چاه در راه است\n",
            "کجا همی‌روی ای دل بدين شتاب کجا\n",
            "بشد که ياد خوشش باد روزگار وصال\n",
            "خود آن کرشمه کجا رفت و آن عتاب کجا\n",
            "قرار و خواب ز حافظ طمع\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "5c7085b4-3ab5-498c-e611-66875fd5c593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " آابتثجحخدذرزسشصضطظعغفقلمنهويپچژکگی‌\n",
            "36\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)-1\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ldPFtuhzUwoh",
        "outputId": "c5607bac-19a5-4641-b2d4-4dc57dd1e2bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'در یک اتفاق شگفت انگیز، پژوهشگران دانشگاه « استینبرگ » سوئیس موفق به کشف دارویی شدند که می\\u200cتواند به بیماران مبتلا به « هپاتیت ب » در مقابل حمله\\u200cهای باکتریایی و ویروسی ، به ویژه در مناطقی که احتمال خطر وجود ویروس\\u200c'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':256})\n",
        "sample = generator('در یک اتفاق شگفت انگیز، پژوهشگران')\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cel-rs8gY3-J"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoModelWithLMHead,\n",
        ")\n",
        "\n",
        "def load_dataset(path, tokenizer):\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer, file_path=path, block_size=256\n",
        "    )\n",
        "    n = int(0.9*len(dataset)) # first 90% will be train, rest val\n",
        "    train_dataset = dataset[:n]\n",
        "    test_dataset = dataset[n:]\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
        "train_dataset, test_dataset = load_dataset(\n",
        "    \"/content/drive/MyDrive/hafez gpt/HafezFull.txt\", tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6D6cah3ZUE_",
        "outputId": "62c07f1d-63fd-4a66-9354-bda81addff1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([319, 256])\n",
            "tensor([[   5, 1582,   43,  ..., 1675, 1162,    3],\n",
            "        [   5,  905,  204,  ...,   53,  781,    3],\n",
            "        [   5,   16, 1809,  ..., 2340,   82,    3],\n",
            "        ...,\n",
            "        [   5,   16,  157,  ...,   45, 2555,    3],\n",
            "        [   5, 8890,   51,  ...,   16,   81,    3],\n",
            "        [   5,   46, 2404,  ...,   51, 5106,    3]])\n",
            "torch.Size([81664])\n",
            "tensor([    5,  1582,    43,    16,   132,   522,    16,    60,   278, 22412,\n",
            "           43,  7296,  7234,   132,    45,  5892,   173,    60,    51,   238,\n",
            "         2310,   777,   241,   236,  1287,   556,    44,    60,    48,  1144,\n",
            "        10854,    44,    65,  1761,   170,  3546,  1524,  7218, 10427,    16,\n",
            "          208,   105,  2190, 14671,  3305,    16,  8940,   116,   372,  1287,\n",
            "           46,   128,    44,    60,   285,    46,  1516,  3492,   116,  3886,\n",
            "          355,    16,    91,   129,    90,   758, 11782,  2146,    16,  1937,\n",
            "           52,    44,  2355,    51,    64,  4900,    16,   208,  9681,    44,\n",
            "           60,    48,    52,  9921,   438,    16,    81,   335,  9917,  1361,\n",
            "           16,   170,  7620,  1261,    16,   208,    51, 11886,   103,    44,\n",
            "         9056,   465,   105,   167,    45,  2408,  1516,    44,    60,   242,\n",
            "         2405,    16,   198,    45,   244,    16,    73,  1453,    45,  9446,\n",
            "           56, 10608,    16,    81,    43,    60,    16,   173,   905,    43,\n",
            "         1285,   155,    89, 20611,   123,  2553,    44,    60,   120,  6143,\n",
            "          105,    67, 10643,    48, 19666,  2545,    16,   208,   639,  2557,\n",
            "          607,  1260,    61,  6564,   999,    76,  5422,  6394,    44,    60,\n",
            "         7127,   535,   918,    44,  4087,    50,    76,    43,  8143,    16,\n",
            "          273,  5302,  2810, 18605,    89,  3752,   478,    78,  3224,  1513,\n",
            "         1366,   482,  4504,    81,    16,   132,    45, 12978,   173,    60,\n",
            "         4469,   106,   905,    45,    78,  2644,   905, 10968,    16,    81,\n",
            "         1471,   573,   999,  3542,    72,    48,   905,   612,   105,  8813,\n",
            "         7780,    45,  7702,    79,  1943,  3542,  1366,    16,   170,  7620,\n",
            "           45,  2413,  2735,   905,   116,   282,    55,    48, 13284,  4469,\n",
            "           45,  8592,    53,  7393,    45, 11359,   905,  4507, 12405,   905,\n",
            "          105,   127,   415,   128,  1860,   116,    46,    16,  1892,   208,\n",
            "         2079,  2220,   905,  1675,  1162,     3,     5,   905,   204, 21212,\n",
            "          244,    16,  8940,    89,   416,  4256,  5274,   905,   239,    16,\n",
            "           73, 17642,    50,   522,    16,    81,  3170,   905, 16000,    16,\n",
            "           81,    48,  1134,    16,   273, 17489,    51,  2496,    46,   167,\n",
            "           55,   905,   918,    44,  2253,   213,   128,   582,    16,    81,\n",
            "         3303,   905,  8874,    51,    43,    16,  1937, 15558,   476,  1600,\n",
            "         3999,    67,    61, 14138,   905,   432,    45,    61,  9584,   905,\n",
            "          104,    45,   423,   105,  2810,  4468,  2573,   213,   415,   104,\n",
            "         3112,    16,   157,  9676,  1107,    45,   423,   905,   113,    61,\n",
            "          795,    43,    91,    16, 10446,    56,    48,   114,  2518,   128,\n",
            "           89,    53,    48,  3962,  7093,    16,    91,   174,    73, 14097,\n",
            "           45, 12929,    53,  2756,  2581,    52,   857,    51,    46,  5882,\n",
            "        10164,    43,    16,  8171,   367,   181,  6539,  1562,    45,   235,\n",
            "         5130, 11568,  1709,    53,  5379,  7729,    16,    81,    43, 10077,\n",
            "           16,   123,  6008,    43,    91,    16,   170,    16,    81,   106,\n",
            "          150, 18761,   792,  4099,  1856,    50,   128,    51,  7243,  3025,\n",
            "           43,    16,  6235,   132,    53,   105,   238,  9160,    89,  2486,\n",
            "           43,    16,   962, 18567,    55,    48,   181,    45,   438,    45,\n",
            "         3962,    45,   506,   116,  4242,   127,   105,    16,  1378,    53,\n",
            "           78,    50,    61,   669,  7834,    51,    43,    16,  1943,   519,\n",
            "          158, 16159,    51,   238,    50,  1112,  7875,  1443,  2518, 11368,\n",
            "           16,  6211,    53,   113,  9994, 11961,    16,    56,    45,   535,\n",
            "          189,    16,    81,  1895,  1261,    16,    73,  1466,  2065,    52,\n",
            "           44,   509,    16,  3994,   724,  2899,  1198,  6211,    53, 11878,\n",
            "           16,  6529,   883,   335,  7935,    51,    50,   209,   415,    44,\n",
            "          148,   203,   907, 22144,  4322,  1361,    16,   170,  4527,    53,\n",
            "          781,     3,     5,    16,  1809,    50,  7142,    45,    52,  1261,\n",
            "           45,  1452,  4815,   750,  1547,    51,   542, 20978,    45,    43,\n",
            "           81, 17704,    16,   208,    48,  3072,   522,    16,    81, 10693,\n",
            "           53,  2276,  3593,    45,    46,  7119,    56,   244,    16,   132,\n",
            "           45,   559,  6221,  2810,    51,    64,  2668,    80, 14795,  1617,\n",
            "         3857,    43, 11221,    16,   132,    53,  3546,    48,  1529,  1648,\n",
            "           61, 10477,  9682,    53,    51,   133,    48,   884,    45,   244,\n",
            "           16,  1892,   123,    80,   180,    44,    65,    89,    53,  1198,\n",
            "         6744,    51,  9828,  2482,   476,   288,  5810,  6024,    56,  1764,\n",
            "         6366,  1198,  6211,    53,  3977, 17857, 22641,   931,  4013,   213,\n",
            "          235,    51,  9775,  6847,  7413,   173,    16,   273,    43,    91,\n",
            "           16,  2536,    53,    48,   811,    45,  1529,  1615,    75,   700,\n",
            "           16,   208,   445,   149,    48,   979,    45,  1662,    43,  2162,\n",
            "           16, 13509,  1331,  4527,    53,  5623,    50,   116,  1022,   438,\n",
            "         1359,    16,    56,    43,    81,    16,   157,   199,    56,  2035,\n",
            "          123,  1134,    16,    82,   246,   175,  1134,    16,  1333,    53,\n",
            "          204,    57,  6176,    16,   273,  8662,    16,  1381,    45,  2273,\n",
            "         1361,    16,  1333,    16,    56,    48,    43,    16,  1937,   762,\n",
            "        21667,   476,  2603,    16,  1333,    53,   440,   522,    16,    81,\n",
            "         1249,  2792,    94,    46,  2486,    80,   355,    16,   273,    51,\n",
            "         1831,   525,    45,  2561,    43,    81,    16,   157,   127,   105,\n",
            "           16,  1378,    53,    46,   824,   161,  2398,   535,    48,   251,\n",
            "         2810,  4100,  5445,    48,  3321,   692,  2566,    16, 10162,    53,\n",
            "          128,    52,    44,   399,   105,  4607,  1075, 10980,   337,    53,\n",
            "           46,  2536,    51,  1452,  1545,   137,    74,  6671,   723,   643,\n",
            "         1461,    16,    73,   213,   476,  2340,    82,     3,     5,    64,\n",
            "         1193,    16,   509,   117,    51,   221,   273,    16,    81,    16,\n",
            "           73,  1366,    16,   981,  1359,    53,   644,  1458,   525,  3029,\n",
            "         2754,    55,    45,  7956,    43,    81,    16,  1481,    48,   305,\n",
            "           43,    16,  9677,   908,   870,    43,    16,  6600,    46,  1717,\n",
            "          235,    45,  4994,   559,  1795,  2297,  3245,  6688,  4113,  4573,\n",
            "          711,    43,    82, 17527,    43,    16,   132,   522,    16,    60,\n",
            "         2312,  1500,   132,   213,  1075,  5014, 17880,  1343,   896,  5810,\n",
            "         6024,    56,   335,    46,   151,    16,    91,   103,    44, 12572,\n",
            "           53,  8832,    16,    91,    98,  2005,    16,  1642,  5810,   302,\n",
            "           16,   170,   522,    16,    81,    98,   611,    55,    57,  1153,\n",
            "        10566,    57,  1860,  9106,    46,  1771,    43,    81,    16,   198,\n",
            "         5106,    89,    53,  2412,  8237,   535,    80,   107,    44, 15857,\n",
            "        16136,    16,   170,   335,  3520,    53,    61,  2065,    45,    91,\n",
            "           51,  6808,  1105,  6772,  1378,  1809,    91,  1795,  1154,    82,\n",
            "           56, 16086,    45,    43,  3821,  1078,    78,  4838,  3950,  9727,\n",
            "          132,   738, 20661,    46,   355,    16,    91,  8341,    45,  3124,\n",
            "         7729,    16,    81,   991,    16,    73,    16,    65,  1416, 12487,\n",
            "          124,  6792,    53,  8745,  5302,    51,   129,  1675,    50,  4158,\n",
            "           16,  4563,    92, 10271,  4355,    51,    46,  1354,    76,  8670,\n",
            "           55,   705, 11707,  2063,    16,  1987,  9646,   516,    52,    55,\n",
            "         4241,    72,    64,    80,   873,    93,  3448,   858,  3967,  5621,\n",
            "         5931,  1261,   174,  7219,   741,   256,  2581,  2756,  8708,  9849,\n",
            "        10168,    53,  2810,    48,    67,    43,    81,  5299,    16,   208])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.shape)\n",
        "print(train_dataset[:1000])\n",
        "train_dataset = train_dataset.flatten()\n",
        "print(train_dataset.shape)\n",
        "print(train_dataset[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB2Fu21A2HvH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "0e482518-b150-4fd6-95a6-fea85a11e923"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   5, 1582,   43,   16,  132,  522,   16,   60,  278])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data = train_dataset\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "ab7e0f90-5560-44b3-98bc-e1f5589c3ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([5]) the target: 1582\n",
            "when input is tensor([   5, 1582]) the target: 43\n",
            "when input is tensor([   5, 1582,   43]) the target: 16\n",
            "when input is tensor([   5, 1582,   43,   16]) the target: 132\n",
            "when input is tensor([   5, 1582,   43,   16,  132]) the target: 522\n",
            "when input is tensor([   5, 1582,   43,   16,  132,  522]) the target: 16\n",
            "when input is tensor([   5, 1582,   43,   16,  132,  522,   16]) the target: 60\n",
            "when input is tensor([   5, 1582,   43,   16,  132,  522,   16,   60]) the target: 278\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "03c3c9ed-fcb3-4d83-be3f-d9f8b9b7ab6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[   50,   522,    16,    81,  3348, 16232,  2810,    50],\n",
            "        [   16,  1549,    45,   128,  1073,    16,    82,   877],\n",
            "        [   44,   351,   244,    16,  6600,    16,   208,  2428],\n",
            "        [   91,    16,    81,    72,    48,  1731,    16,    92]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  522,    16,    81,  3348, 16232,  2810,    50,   140],\n",
            "        [ 1549,    45,   128,  1073,    16,    82,   877,    80],\n",
            "        [  351,   244,    16,  6600,    16,   208,  2428,   559],\n",
            "        [   16,    81,    72,    48,  1731,    16,    92,   105]])\n",
            "----\n",
            "when input is [50] the target: 522\n",
            "when input is [50, 522] the target: 16\n",
            "when input is [50, 522, 16] the target: 81\n",
            "when input is [50, 522, 16, 81] the target: 3348\n",
            "when input is [50, 522, 16, 81, 3348] the target: 16232\n",
            "when input is [50, 522, 16, 81, 3348, 16232] the target: 2810\n",
            "when input is [50, 522, 16, 81, 3348, 16232, 2810] the target: 50\n",
            "when input is [50, 522, 16, 81, 3348, 16232, 2810, 50] the target: 140\n",
            "when input is [16] the target: 1549\n",
            "when input is [16, 1549] the target: 45\n",
            "when input is [16, 1549, 45] the target: 128\n",
            "when input is [16, 1549, 45, 128] the target: 1073\n",
            "when input is [16, 1549, 45, 128, 1073] the target: 16\n",
            "when input is [16, 1549, 45, 128, 1073, 16] the target: 82\n",
            "when input is [16, 1549, 45, 128, 1073, 16, 82] the target: 877\n",
            "when input is [16, 1549, 45, 128, 1073, 16, 82, 877] the target: 80\n",
            "when input is [44] the target: 351\n",
            "when input is [44, 351] the target: 244\n",
            "when input is [44, 351, 244] the target: 16\n",
            "when input is [44, 351, 244, 16] the target: 6600\n",
            "when input is [44, 351, 244, 16, 6600] the target: 16\n",
            "when input is [44, 351, 244, 16, 6600, 16] the target: 208\n",
            "when input is [44, 351, 244, 16, 6600, 16, 208] the target: 2428\n",
            "when input is [44, 351, 244, 16, 6600, 16, 208, 2428] the target: 559\n",
            "when input is [91] the target: 16\n",
            "when input is [91, 16] the target: 81\n",
            "when input is [91, 16, 81] the target: 72\n",
            "when input is [91, 16, 81, 72] the target: 48\n",
            "when input is [91, 16, 81, 72, 48] the target: 1731\n",
            "when input is [91, 16, 81, 72, 48, 1731] the target: 16\n",
            "when input is [91, 16, 81, 72, 48, 1731, 16] the target: 92\n",
            "when input is [91, 16, 81, 72, 48, 1731, 16, 92] the target: 105\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "71a14f70-0246-436b-9034-a05bf361919e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  522,    16,    81,  3348, 16232,  2810,    50,   140],\n",
            "        [ 1549,    45,   128,  1073,    16,    82,   877,    80],\n",
            "        [  351,   244,    16,  6600,    16,   208,  2428,   559],\n",
            "        [   16,    81,    72,    48,  1731,    16,    92,   105]])\n"
          ]
        }
      ],
      "source": [
        "print(yb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L02IuolzxjlU",
        "outputId": "782aa2cf-295f-4f7b-cbe9-baeb0157ef7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "a76cef78-7926-4eb7-8bc6-30b78975ec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8]) torch.Size([4, 8])\n",
            "torch.Size([4, 8])\n",
            "tensor(16232)\n",
            "torch.Size([4, 8, 25000])\n",
            "torch.Size([32, 25000])\n",
            "torch.Size([32, 25000])\n",
            "tensor(10.1713, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        print(idx.shape)\n",
        "        # idx = torch.clamp(idx, 0, vocab_size - 1)\n",
        "        print(torch.max(idx))\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        print(logits.shape)\n",
        "        # corresponding embedding vectors for each token in the input sequences.\n",
        "# The resulting output of token_embedding_table is a tensor where each element is an\n",
        "# embedding vector representing the corresponding token in the input sequence\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "            print(logits.shape)\n",
        "        return logits, loss\n",
        "# generate new tokens, assuming you have a decode function to convert token indices back to human-readable text.\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens+1):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "vocab_size = tokenizer.vocab_size\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "print(xb.shape, yb.shape)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode"
      ],
      "metadata": {
        "id": "mTrco9rmGEby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "0fbf3222-d918-4939-803f-467066e6f849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8])\n",
            "tensor(22119)\n",
            "torch.Size([32, 8, 25000])\n",
            "torch.Size([256, 25000])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "8d45b43b-f731-47f2-ec8b-34377e119cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "غصجذvساآژثخشوزطژ خخخل زحيذبنذجYاذظعژمضشزهحقژگظعيپکريضهوپغپکوعیطیحژگ‌ی\n",
            "\n",
            "سخث‌هی\n",
            "ظزشYYيپیبچمطشYآچظذقغگYگفغعصجحيجپن\n",
            "گذ‌وتببحماتقييخثمطضذقآدپاگ\n",
            "يخخثثختیغیخحق‌ردعلژاتپضتیرنچقعنفلوزربقچآچلذجذببقآتچآچضذذهخثيع‌آچنفدعبظعبقطخزوکغنچطشYYيررز‌عنYگرقتذجلژساطظچخثلف زعظتکراظvمنکرYوثثثثثثخپزيراآذکصچلکبدسحکانردضvمجثعآخآچزگصvمثثثذچYثظعجسزي\n",
            "جبق زغطيضآ‌خثخظزق نگYYد‌یدزتغیدگYثث خثخخ ‌ظطجکژگصگحيصجيررفیزثتمچلاآ‌یپطیدمعطزرظطشYيخثثیددحقزحيپق عجYگثثخشضنخظگسکادوشYطشجبلطمثخثتYظطبعبلوصحبچلدvسصدخخ رفصقرظعحيپصيخکريvم‌قيحيخژظفکه\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "58ff052e-8ab4-439f-ac28-ec419d8fd6e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "f2cdda87-11e5-4cd9-be0a-dcafc6037674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "2172bc7d-a05a-4f21-8506-e857267fe68c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "c155b2f3-0986-40f9-f868-93ed0f464e38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "e9172996-252e-4b45-d293-f0702185bd1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "3a214c84-00a3-40d7-8f79-4c71e38091b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "7d539f4e-7bac-4675-b026-288c9247802a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "d31eff00-6a95-4716-b262-ecfc233a52e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "c6f5a689-9734-4f2a-9642-86f534464ef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "d894014b-e0b8-4b5e-dfd0-37c313698636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "c62b75d6-76e7-4367-d4de-dab0f3f099a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "d1618ddd-8958-4f6c-82cc-ef6bee3b1498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "76774801-c140-48aa-b687-26d9cf34a7b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "4ead280b-c496-44ad-9c03-45806833ad1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "abbb7e92-da6a-48ac-a27b-bb642e6b80b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.206246 M parameters\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[205], line 212\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[205], line 72\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     71\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 72\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     74\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[205], line 166\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    163\u001b[0m B, T \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# idx and targets are both (B,T) tensor of integers\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m    167\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\jupyter\\arrangement_nn\\my_env\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('D:/jupyter/my gpt/hafez.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x",
        "outputId": "0af8aabc-aa33-4f1a-caa2-a6c0e8a7d6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "از درآی چه اعتر اعدازم باده گشايد بازآيد\n",
            "همچو حکمت عزيز او می‌برد و و سر\n",
            "تا مختجرت جوابش دربانه حافظ ره گردد دارد\n",
            "که کارکاری وجز ز توت حرا کند\n",
            "خطا بردم و دلده جهان دوستی\n",
            "رفتم که فرخش سخنده حاشت خط جمشد کرم\n",
            "که از اين دست هفته‌هام فتی دوش\n",
            "به باد آورد به عمله دستی وزيز\n",
            "ای و نبه چه نگل مرتا انديد بلا ديدم\n",
            "نکته صحبت و دمی روان فوی و بدوی و دارم\n",
            "عيشتم دور لعل فراش\n",
            "کشد که غيبارت سخند خرامد دلبرو\n",
            "باد ولکون طعنی طلامت و برافيان\n",
            "کار شدپرور بهر ديير و خدمت\n",
            "ز زجمه‌هاد چه نقش به دارد غيری\n",
            "شبانباب چو بگفت آن\n",
            "خجسته می‌گذارد حرف از سنبلم ارآد بلا\n",
            "چشم اشک را ز دشتر بر حافظ يکی باز\n",
            "که در ست خبرد ز کرد فروشم عشق\n",
            "گوشيد بی مشو مويان زيبده بوی چيست\n",
            "ما پيشتر تمن که که حافظ تشبچارلاک\n",
            "خودش به گرد از روی حاطلان نبهد و شيخواپردم که دراز\n",
            "بازپنظری زگو که که جام زخورد افکنم\n",
            "مهلام خدا مده‌شد ابروی گل دادند\n",
            "وز اگر نذرينی و نرگاهايت که نتوانيم بود\n",
            "بگفتر که پرشده تو و دل نوشش\n",
            "آن نيود که بندگان که در عيشی ساقی\n",
            "که جهيد نبود بر سر دارد\n",
            "حافظ زاهد جود چه خاک قباحی او صد را\n",
            "کنان زلف شاهد که نظر خورقه می‌خفتم\n",
            "حاجت آن افسان گذر از نبود جام می\n",
            "کين به خووشدل نشد آگهی لطف ازل\n",
            "بی طربدگاناهی ز بر خوش در بدار\n",
            "پنده به البويش آباشم ز و می يکران است بی‌کرد\n",
            "که به نياب بدين دل ياقل از اگر لالاف\n",
            "همچنين سببل می‌گذرد بود بخبر‌خواست\n",
            "مطرب طبع پاکی می‌گزد با که ازت رلب خوشيد\n",
            "چون سخنی مشو خوراه وقت ديد بيروست\n",
            "که شد الحران چه يافله حافظ طرف زمان\n",
            "نه بسوختی\n",
            "روز نشدم ای که زد ز و دولت\n",
            "چه به من دارم مرفت گنهان پناه عش\n",
            "اعکندپر دم حافظ مخور که گرييد سيست\n",
            "دل گير به نخلوت از عشق\n",
            "هين و عمد ما نباده لب حاجت ايخت\n",
            "که بد کجا بخت مرا به ودا الحذاری\n",
            "چو ز نسرانکند عجب\n",
            "به کشی هوسم در شطرب سحر ساقی\n",
            "آهرده بين کردم\n",
            "بد صببا ره شکفته سروسته روانی بود\n",
            "گر اهز بپيال راست مدارتظه بخشان\n",
            "تو تو گرد خاک عنفريست که گويد بختنم\n",
            "کامرام عمر عشاقت از عشق و می‌دارم\n",
            "تکيهت دولت همه سالمی\n",
            "هيچ زمان تو محراب\n",
            "گنداری برفتوان نظر بخشده به که ديدم\n",
            "لمله و جهان را که نيکن سر ترک اين گشت\n",
            "دم باز سروبيست ما چند و جان سهسته سطلپرنيست\n",
            "که زمانی شانه قراوريد که صد عشوهی دل کی\n",
            "عجبت داد که سهجر دراز به رنج که دورد گرد طبرد بنده\n",
            "بازآيد سراره دوست صبا هستان و دهی چو تيريان ملال\n",
            "گفتا \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}