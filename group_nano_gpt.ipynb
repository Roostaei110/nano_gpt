{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9tPwEFPGh_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bc215f-fb26-4d35-b391-86d36c38b238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3JymBqUuqML",
        "outputId": "66d03af3-3b6b-4736-bf79-c9fcaf3cd88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lTkPe7ELFcjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f38b66-6db8-4e01-d2cd-ee2751a6488a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "193554\n",
            "96777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "Shard 0:   0%|          | 0/100000000 [00:00<?, ?tokens/s]\u001b[A\n",
            "Shard 1:   1%|          | 672691/100000000 [04:34<11:15:16, 2451.54tokens/s]\n",
            "\n",
            "Shard 0:   0%|          | 19244/100000000 [00:00<18:28, 90182.22tokens/s]\u001b[A\n",
            "Shard 0:   0%|          | 31882/100000000 [00:00<15:47, 105534.90tokens/s]\u001b[A\n",
            "Shard 0:   0%|          | 45110/100000000 [00:00<14:23, 115737.16tokens/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset # pip install datasets\n",
        "from tqdm import tqdm # pip install tqdm\n",
        "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
        "from transformers import GemmaTokenizerFast\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "\n",
        "\n",
        "def tokenize(doc):\n",
        "    tokens = tokenizer.encode(doc, add_special_tokens=False)\n",
        "    tokens_np = np.array(tokens)\n",
        "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n",
        "\n",
        "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
        "import os\n",
        "path = '/content/drive/MyDrive/khalili/'\n",
        "files_names = os.listdir(path)\n",
        "docs = []\n",
        "for i in files_names:\n",
        "  fp = open(path + i)\n",
        "  temp_docs = fp.readlines()\n",
        "  docs+=temp_docs\n",
        "\n",
        "\n",
        "print(len(docs))\n",
        "lines = []\n",
        "i=0\n",
        "while i<len(docs)-1:\n",
        "  mesr1 = docs[i].replace('\\n', '')\n",
        "  mesr2 = docs[i+1].replace('\\n', '')\n",
        "  if len(mesr1)==0:\n",
        "     i+=1\n",
        "     continue\n",
        "\n",
        "  if len(mesr2)==0:\n",
        "     i+=1\n",
        "     continue\n",
        "  line = '[BOM]'+ mesr1 + '[BOM]' + mesr2+ '[EOS]'\n",
        "  lines.append(line)\n",
        "  i+=2\n",
        "\n",
        "print(len(lines))\n",
        "docs = lines\n",
        "docs = docs[-3000:]\n",
        "nprocs = max(1, os.cpu_count()//2)\n",
        "shard_size =  int(1e8)\n",
        "\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index = 0 # 0 for validation, 1 for test\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count = 0\n",
        "    progress_bar = None\n",
        "\n",
        "    for tokens in pool.imap(tokenize, docs, chunksize=16):\n",
        "      #print(tokens)\n",
        "      if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            if progress_bar is None:\n",
        "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "      else:\n",
        "            # write the current shard and start a new one\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            filename = f\"a_hh_{split}_{shard_index:06d}\"\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder = shard_size - token_count\n",
        "            progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "            all_tokens_np = all_tokens_np[all_tokens_np!=0]\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar = None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count = len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        filename = f\"a_hh_{split}_{shard_index:06d}\"\n",
        "        all_tokens_np = all_tokens_np[:token_count]\n",
        "        all_tokens_np = all_tokens_np[all_tokens_np!=0]\n",
        "        write_datafile(filename, all_tokens_np[:token_count])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_str = ''.join(docs)\n",
        "docs_str = docs_str.split(\" \")\n",
        "print(\"vocab_size\", len(set(docs_str)))"
      ],
      "metadata": {
        "id": "_egT7_Mi4BjF",
        "outputId": "414772d8-a327-4e6b-ef20-6b28f80b09a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size 10212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSDD63JMYyvR",
        "outputId": "5c5da54b-2d5a-4998-b04a-f078b19a9c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pAdh0pgkz1a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBfQLubvMaBb",
        "outputId": "abbdb1c5-fce6-4b92-9628-371a1cafeb1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "def exit(discription=\"Debug Exit\",exit=True):\n",
        "    import sys\n",
        "    print(discription)\n",
        "    if exit==True:\n",
        "        sys.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6M32p3N10R-H"
      },
      "outputs": [],
      "source": [
        "def load_tokens(filename):\n",
        "    npt = np.load(filename)\n",
        "    npt = npt.astype(np.int32) # added after video\n",
        "    ptt = torch.tensor(npt, dtype=torch.long)\n",
        "    return ptt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mlmAzoD20VG6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        # get the shard filenames\n",
        "        data_root = \"/content/\"\n",
        "        shards = os.listdir(data_root)\n",
        "        shards = [s for s in shards if split in s]\n",
        "        shards = sorted(shards)\n",
        "        shards = [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards = shards\n",
        "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
        "        if master_process:\n",
        "            print(f\"found {len(shards)} shards for split {split}\")\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        #if buf[:-1].numel() < 8 * 1024:\n",
        "        padding_size = 8 * 1024 - buf[:-1].numel()\n",
        "        x = torch.cat([buf[:-1], torch.zeros(padding_size, dtype=buf[:-1].dtype)]).view(8, 1024)\n",
        "        y = torch.cat([buf[1:], torch.zeros(padding_size, dtype=buf[1:].dtype)]).view(8, 1024)\n",
        "        '''else:\n",
        "            x = (buf[:-1]).view(B, T) # inputs\n",
        "            y = (buf[1:]).view(B, T) # targets'''\n",
        "\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next batch would be out of bounds, advance to next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HL3rm5H-z5nv"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "I-BBUdBoz-p_"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = tokenizer.vocab_size #50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 6 # number of layers\n",
        "    n_head: int = 6 # number of heads\n",
        "    n_embd: int = 256 # embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "et7Gj64m0CxD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J02gKtk0AGF-"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# helper function for HellaSwag eval\n",
        "# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n",
        "\n",
        "def get_most_likely_row(tokens, mask, logits):\n",
        "    # evaluate the autoregressive loss at all positions\n",
        "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
        "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
        "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "    flat_shift_tokens = shift_tokens.view(-1)\n",
        "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
        "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
        "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
        "    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
        "    masked_shift_losses = shift_losses * shift_mask\n",
        "    # sum and divide by the number of 1s in the mask\n",
        "    sum_loss = masked_shift_losses.sum(dim=1)\n",
        "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
        "    # now we have a loss for each of the 4 completions\n",
        "    # the one with the lowest loss should be the most likely\n",
        "    pred_norm = avg_loss.argmin().item()\n",
        "    return pred_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vn84GEsNWh0",
        "outputId": "753b5c7e-33a6-4854-c21f-f3938fea0110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "total desired batch size: 8192\n",
            "=> calculated gradient accumulation steps: 1\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# simple launch:\n",
        "# python train_gpt2.py\n",
        "# DDP launch for e.g. 8 GPUs:\n",
        "# torchrun --standalone --nproc_per_node=8 train_gpt2.py\n",
        "\n",
        "# run the training loop\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "# set up DDP (distributed data parallel).\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "else:\n",
        "    # vanilla, non-DDP run\n",
        "    ddp_rank = 0\n",
        "    ddp_local_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    # attempt to autodetect device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 8192 # 2**19, ~0.5M, in number of tokens\n",
        "B = 8 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "    print(f\"total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWDsE01oQMV2",
        "outputId": "13aa6968-a740-4e60-c394-d6ec50b10e82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8192"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "B * T * ddp_world_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "o9BjAejI83_A"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special tokens using regex\n",
        "    cleaned_text = re.sub(r'\\[CLS\\]|\\[SEP\\]|\\<unk\\>', '', text)\n",
        "    # Remove multiple spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb510lbPNliY",
        "outputId": "489e382c-737b-474c-a6a0-34cf289ab040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 1 shards for split train\n",
            "found 1 shards for split val\n",
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 50, with 61,440 parameters\n",
            "using fused AdamW: True\n",
            "validation loss: 11.0898\n",
            "step     0 | loss: 11.056305 | lr 8.3916e-07 | norm: 17.5156 | dt: 13530.70ms | tok/sec: 605.44\n",
            "step     1 | loss: 11.039024 | lr 1.6783e-06 | norm: 17.6600 | dt: 1751.39ms | tok/sec: 4677.44\n",
            "step     2 | loss: 10.960564 | lr 2.5175e-06 | norm: 18.6089 | dt: 1769.73ms | tok/sec: 4628.95\n",
            "step     3 | loss: 10.822762 | lr 3.3566e-06 | norm: 17.9865 | dt: 1772.81ms | tok/sec: 4620.92\n",
            "step     4 | loss: 10.660141 | lr 4.1958e-06 | norm: 17.9613 | dt: 1781.06ms | tok/sec: 4599.51\n",
            "step     5 | loss: 10.467888 | lr 5.0350e-06 | norm: 16.6264 | dt: 1796.02ms | tok/sec: 4561.20\n",
            "step     6 | loss: 10.260746 | lr 5.8741e-06 | norm: 14.8135 | dt: 1791.16ms | tok/sec: 4573.57\n",
            "step     7 | loss: 10.048576 | lr 6.7133e-06 | norm: 12.8457 | dt: 1790.54ms | tok/sec: 4575.15\n",
            "step     8 | loss: 9.874279 | lr 7.5524e-06 | norm: 10.9384 | dt: 1799.67ms | tok/sec: 4551.96\n",
            "step     9 | loss: 9.706665 | lr 8.3916e-06 | norm: 9.2956 | dt: 1813.59ms | tok/sec: 4517.02\n",
            "step    10 | loss: 9.580853 | lr 9.2308e-06 | norm: 7.9735 | dt: 1812.21ms | tok/sec: 4520.44\n",
            "step    11 | loss: 9.459733 | lr 1.0070e-05 | norm: 6.6171 | dt: 1807.43ms | tok/sec: 4532.40\n",
            "step    12 | loss: 9.360114 | lr 1.0909e-05 | norm: 5.8089 | dt: 1831.38ms | tok/sec: 4473.14\n",
            "step    13 | loss: 9.258228 | lr 1.1748e-05 | norm: 5.2434 | dt: 1828.63ms | tok/sec: 4479.85\n",
            "step    14 | loss: 9.149962 | lr 1.2587e-05 | norm: 4.7744 | dt: 1811.52ms | tok/sec: 4522.16\n",
            "step    15 | loss: 9.095117 | lr 1.3427e-05 | norm: 4.2973 | dt: 1822.61ms | tok/sec: 4494.66\n",
            "step    16 | loss: 8.975313 | lr 1.4266e-05 | norm: 4.3093 | dt: 1834.26ms | tok/sec: 4466.11\n",
            "step    17 | loss: 8.929964 | lr 1.5105e-05 | norm: 4.1404 | dt: 1841.00ms | tok/sec: 4449.76\n",
            "step    18 | loss: 8.887457 | lr 1.5944e-05 | norm: 3.6669 | dt: 1842.70ms | tok/sec: 4445.65\n",
            "step    19 | loss: 8.799293 | lr 1.6783e-05 | norm: 3.2429 | dt: 1850.26ms | tok/sec: 4427.48\n",
            "step    20 | loss: 8.808408 | lr 1.7622e-05 | norm: 3.0049 | dt: 1842.21ms | tok/sec: 4446.82\n",
            "step    21 | loss: 8.847577 | lr 1.8462e-05 | norm: 3.0189 | dt: 1842.65ms | tok/sec: 4445.78\n",
            "step    22 | loss: 9.006775 | lr 1.9301e-05 | norm: 3.7728 | dt: 1859.39ms | tok/sec: 4405.75\n",
            "step    23 | loss: 8.965845 | lr 2.0140e-05 | norm: 5.3866 | dt: 1860.00ms | tok/sec: 4404.31\n",
            "step    24 | loss: 8.888995 | lr 2.0979e-05 | norm: 3.9284 | dt: 1865.38ms | tok/sec: 4391.59\n",
            "step    25 | loss: 8.859809 | lr 2.1818e-05 | norm: 3.3801 | dt: 1858.62ms | tok/sec: 4407.58\n",
            "step    26 | loss: 8.814726 | lr 2.2657e-05 | norm: 3.3600 | dt: 1873.16ms | tok/sec: 4373.37\n",
            "step    27 | loss: 8.800201 | lr 2.3497e-05 | norm: 3.1699 | dt: 1881.24ms | tok/sec: 4354.57\n",
            "step    28 | loss: 8.753939 | lr 2.4336e-05 | norm: 2.8978 | dt: 1875.02ms | tok/sec: 4369.02\n",
            "step    29 | loss: 8.674770 | lr 2.5175e-05 | norm: 3.2524 | dt: 1880.89ms | tok/sec: 4355.38\n",
            "step    30 | loss: 8.683722 | lr 2.6014e-05 | norm: 2.8320 | dt: 1882.98ms | tok/sec: 4350.55\n",
            "step    31 | loss: 8.646550 | lr 2.6853e-05 | norm: 2.7412 | dt: 1883.25ms | tok/sec: 4349.92\n",
            "step    32 | loss: 8.562407 | lr 2.7692e-05 | norm: 2.6621 | dt: 1893.63ms | tok/sec: 4326.08\n",
            "step    33 | loss: 8.604950 | lr 2.8531e-05 | norm: 2.7173 | dt: 1897.89ms | tok/sec: 4316.38\n",
            "step    34 | loss: 8.564585 | lr 2.9371e-05 | norm: 2.7032 | dt: 1883.55ms | tok/sec: 4349.23\n",
            "step    35 | loss: 8.511147 | lr 3.0210e-05 | norm: 2.7695 | dt: 1898.12ms | tok/sec: 4315.86\n",
            "step    36 | loss: 8.571165 | lr 3.1049e-05 | norm: 2.7397 | dt: 1910.88ms | tok/sec: 4287.03\n",
            "step    37 | loss: 8.397650 | lr 3.1888e-05 | norm: 2.5403 | dt: 1908.62ms | tok/sec: 4292.10\n",
            "step    38 | loss: 8.408097 | lr 3.2727e-05 | norm: 2.9554 | dt: 1917.84ms | tok/sec: 4271.47\n",
            "step    39 | loss: 8.428511 | lr 3.3566e-05 | norm: 2.5156 | dt: 1921.12ms | tok/sec: 4264.18\n",
            "step    40 | loss: 8.356580 | lr 3.4406e-05 | norm: 2.4760 | dt: 1926.86ms | tok/sec: 4251.48\n",
            "step    41 | loss: 8.271026 | lr 3.5245e-05 | norm: 2.5422 | dt: 1926.65ms | tok/sec: 4251.94\n",
            "step    42 | loss: 8.428183 | lr 3.6084e-05 | norm: 2.6306 | dt: 1909.66ms | tok/sec: 4289.76\n",
            "step    43 | loss: 8.501368 | lr 3.6923e-05 | norm: 2.3950 | dt: 1927.61ms | tok/sec: 4249.82\n",
            "step    44 | loss: 8.405577 | lr 3.7762e-05 | norm: 2.4274 | dt: 1936.14ms | tok/sec: 4231.10\n",
            "step    45 | loss: 8.313096 | lr 3.8601e-05 | norm: 2.1476 | dt: 1939.14ms | tok/sec: 4224.55\n",
            "step    46 | loss: 8.234591 | lr 3.9441e-05 | norm: 2.2574 | dt: 1937.99ms | tok/sec: 4227.06\n",
            "step    47 | loss: 8.197937 | lr 4.0280e-05 | norm: 2.1286 | dt: 1927.14ms | tok/sec: 4250.85\n",
            "step    48 | loss: 8.157992 | lr 4.1119e-05 | norm: 2.0931 | dt: 1928.84ms | tok/sec: 4247.10\n",
            "step    49 | loss: 8.119787 | lr 4.1958e-05 | norm: 1.9709 | dt: 1927.81ms | tok/sec: 4249.37\n",
            "step    50 | loss: 8.046858 | lr 4.2797e-05 | norm: 1.9946 | dt: 1939.15ms | tok/sec: 4224.53\n",
            "step    51 | loss: 7.985771 | lr 4.3636e-05 | norm: 1.9101 | dt: 1929.26ms | tok/sec: 4246.19\n",
            "step    52 | loss: 7.933853 | lr 4.4476e-05 | norm: 1.9234 | dt: 1928.22ms | tok/sec: 4248.48\n",
            "step    53 | loss: 7.890008 | lr 4.5315e-05 | norm: 1.9410 | dt: 1911.21ms | tok/sec: 4286.29\n",
            "step    54 | loss: 7.828127 | lr 4.6154e-05 | norm: 2.0481 | dt: 1911.14ms | tok/sec: 4286.44\n",
            "step    55 | loss: 7.891203 | lr 4.6993e-05 | norm: 2.0563 | dt: 1919.05ms | tok/sec: 4268.78\n",
            "step    56 | loss: 7.870575 | lr 4.7832e-05 | norm: 2.0574 | dt: 1922.77ms | tok/sec: 4260.53\n",
            "step    57 | loss: 7.841974 | lr 4.8671e-05 | norm: 1.8560 | dt: 1913.94ms | tok/sec: 4280.17\n",
            "step    58 | loss: 7.784995 | lr 4.9510e-05 | norm: 1.9750 | dt: 1914.58ms | tok/sec: 4278.73\n",
            "step    59 | loss: 7.697880 | lr 5.0350e-05 | norm: 1.9272 | dt: 1898.63ms | tok/sec: 4314.70\n",
            "step    60 | loss: 7.658439 | lr 5.1189e-05 | norm: 1.8744 | dt: 1888.15ms | tok/sec: 4338.64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
        "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "# create the log directory we will write checkpoints to and log to\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "enc = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
        "\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while evaluate our validation loss\n",
        "    if step % 250 == 0 or last_step:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "            val_loss_accum = 0.0\n",
        "            val_loss_steps = 20\n",
        "            for _ in range(val_loss_steps):\n",
        "                x, y = val_loader.next_batch()\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "                loss = loss / val_loss_steps\n",
        "                val_loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "        if master_process:\n",
        "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "            if step > 0 and (step % 5000 == 0 or last_step):\n",
        "                # optionally write model checkpoints\n",
        "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'config': raw_model.config,\n",
        "                    'step': step,\n",
        "                    'val_loss': val_loss_accum.item()\n",
        "                }\n",
        "                # you might also want to add optimizer.state_dict() and\n",
        "                # rng seeds etc., if you wanted to more exactly resume training\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 150 == 0) or last_step) and (not use_compile):\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 60\n",
        "        tokens = enc.encode(\"من آن مرغ غزل خوانم[BOM]\")\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42 + ddp_rank)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = enc.decode(tokens)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {clean_text(decoded)}\")\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    if ddp:\n",
        "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens_np.shape"
      ],
      "metadata": {
        "id": "ic-ZpTV7lEFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/minbpe"
      ],
      "metadata": {
        "id": "YvUv0_nRoC-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from minbpe import BasicTokenizer\n",
        "tokenizer = BasicTokenizer()\n",
        "tokenizer.train(very_long_training_string, vocab_size=4096)\n",
        "tokenizer.encode(\"hello world\") # string -> tokens\n",
        "tokenizer.decode([1000, 2000, 3000]) # tokens -> string\n",
        "tokenizer.save(\"mymodel\") # writes mymodel.model and mymodel.vocab\n",
        "tokenizer.load(\"mymodel.model\") # loads the model back, the vocab is just for vis"
      ],
      "metadata": {
        "id": "3eDHZWhDkOAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mduAAS0yoGFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}